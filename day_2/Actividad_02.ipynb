{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6675efc",
   "metadata": {},
   "source": [
    "# Introduccion al Reconocimiento del Habla - Tutorial ESPnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfaf024",
   "metadata": {},
   "source": [
    "Ref:\n",
    "- https://colab.research.google.com/drive/1L85G7jdhsI1QKs2o0qCGEbhm5X4QV2zN?usp=sharing\n",
    "\n",
    "Autor Original: Shinji Watanabe\n",
    "    \n",
    "ESPnet is un tolkit para procesamiento de voz end-to-end (fin-a-fin), inicialmente aplicado al reconocimiento del habla fin-a-fin\n",
    "y la sintetizacion de voz (texto-a-voz) fin-a-fin. En la actualidad este toolkit ha sido extendido a otros aplicaciones del procesamiento\n",
    "del habla. ESPnet usa PyTorch como motor principal para el aprendizaje profundo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6c4fa",
   "metadata": {},
   "source": [
    "## Ejemplo del reconocimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d9d12",
   "metadata": {},
   "source": [
    "- ESPnet cuenta con varias aplicaciones dedicadas al procesamiento de voz y tiene modelos preentrenados\n",
    "- Podemos verificar los modelos en [espnet_model_zoo](https://github.com/espnet/espnet_model_zoo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad82983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c913a33",
   "metadata": {},
   "source": [
    "Ahora escogemos el idioma de nuestro reconocedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb97c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "fs = 16000\n",
    "tag = 'Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f5105",
   "metadata": {},
   "source": [
    "### Inicializacion del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af00d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "\n",
    "d = ModelDownloader(\"/content/drive/MyDrive/Data_NB/pretrained\")\n",
    "# It may takes a while to download and build models\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack(tag),\n",
    "    device=\"cuda\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.0,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea72806",
   "metadata": {},
   "source": [
    "### Reconocimiento de nuestras grabaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f404fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+git://github.com/ricardodeazambuja/colab_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_utils import getAudio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from librosa import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e610053",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech, fs = getAudio()\n",
    "dtype = speech.dtype\n",
    "speech = speech.astype(np.float32) / (np.iinfo(dtype).max + 1)\n",
    "speech = resample(speech, fs, 16000)\n",
    "plt.plot(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60beeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = 'Today I will share information about deep learning'\n",
    "nbests = speech2text(speech)\n",
    "\n",
    "text, *_ = nbests[0]\n",
    "print(f\"ASR hypothesis: {text_normalizer(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874924c",
   "metadata": {},
   "source": [
    "## Instalacion completa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c15736",
   "metadata": {},
   "source": [
    "Para el proceso de Preprocesamiento, entrenamiento, inferencia, evaluacion, necesitamos contar con una instalacion completa  \n",
    "del toolkit.\n",
    "Recuerden que este toolkit solo funciona en **Linux** (Ubuntu de preferencia.)\n",
    "Muchas de las herramientas no estan disponibles para **Windows**. Asi que no se recomienda su uso completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar ESPnet\n",
    "%cd /content/drive/MyDrive/Data_NB/\n",
    "# OS setup\n",
    "!cat /etc/os-release\n",
    "!apt-get install -qq bc tree sox\n",
    "!chmod a+x espnet/utils/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# espnet setup\n",
    "!pip install chainer==6.0.0\n",
    "# !git clone --depth 5 https://github.com/espnet/espnet\n",
    "# !cd espnet; pip install -q -e .\n",
    "\n",
    "# download pre-compiled warp-ctc and kaldi tools\n",
    "!espnet/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=13Y4tSygc8WtqzvAVGK_vRV9GlV7TRC0w\" espnet/tools tar.gz > /dev/null\n",
    "!cd espnet/tools && bash installers/install_sph2pipe.sh\n",
    "\n",
    "# make dummy activate\n",
    "!rm espnet/tools/activate_python.sh \n",
    "!touch espnet/tools/activate_python.sh\n",
    "!echo \"setup done.\"\n",
    "%rm -rf espnet/tools/kaldi\n",
    "!git clone https://github.com/kaldi-asr/kaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb90d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la correcta instalacion\n",
    "%cd /content/espnet/tools\n",
    "!. ./activate_python.sh; python3 check_install.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c7325",
   "metadata": {},
   "source": [
    "## Empleando una receta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea915139",
   "metadata": {},
   "source": [
    "ESPnet cuenta con un numero de recetas (73 en total hasta el dia 16 de Sept. 2021).\n",
    "\n",
    "Para los siguientes pasos, utilizaremos la guia general descrita en: https://espnet.github.io/espnet/espnet2_tutorial.html#recipes-using-espnet2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c51bc1",
   "metadata": {},
   "source": [
    "### CMU AN4\n",
    "\n",
    "Para este club emplearemos como muestra la receta de la base de datos AN4.  \n",
    "Es una tarea de pequeña escala principalmente destinada a evaluaciones de la plataforma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fe913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos movemos al directorio\n",
    "%cd /content/drive/MyDrive/Data_NB/espnet/egs2/an4/asr1\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75d56e",
   "metadata": {},
   "source": [
    "La ejecucion de la receta se realiza a traves de `run.sh` que llama a `asr.sh`. Este archivo completa el experimento del reconocimiento del habla, incluyendo preparacion de los datos, entrenamiento, inferencia y puntuacion. En total se cuenta con 15 diferentes etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ca8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64875400",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x local/*.sh\n",
    "!chmod a+x utils/*.sh\n",
    "!chmod a+x utils/*.pl\n",
    "!chmod a+x step/*.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b64737",
   "metadata": {},
   "source": [
    "### Etapa 1: Preparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf92ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 1 --stop_stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fea571",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfafdf",
   "metadata": {},
   "source": [
    "### Etapa 2: Perturbacion de la velociadad\n",
    "\n",
    "Para este ejercicio, no emplearemos la perturbacion. Usualmente empleada para generar nuevos datos\n",
    "(incremento de la base de datos, o data augmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511e0fe",
   "metadata": {},
   "source": [
    "### Etapa 3: Formato de wav.scp: data/ -> dump/raw\n",
    "Convertimos los datos en un formato especifico (para este caso: flac) que hace un uso eficiente de la memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 3 --stop_stage 3 --nj 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9f377",
   "metadata": {},
   "source": [
    "### Etapa 4: Eliminar archivos de corta o larga duracion\n",
    "Archivos de muy corta o larga duracion pueden ser dañinos al entrenamiendo. Estos archivos deben ser retirados de la lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34273c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 4 --stop_stage 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ec436",
   "metadata": {},
   "source": [
    "### Etapa 5: Generacion de una lista de tokens (caracteres) usando BPE.\n",
    "Esta es una etapa importante para el procesamiento de texto. Hacemos un diccionario basado en carateres en inglés para este ejercicio.  \n",
    "Para esto, empleamos la herramienta `sentecepiece` desarrollada por Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 5 --stop_stage 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82445c8",
   "metadata": {},
   "source": [
    "Revisamos el diccionario, en el cual podremos encontrar caracteres especiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/token_list/bpe_unigram30/tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7e862",
   "metadata": {},
   "source": [
    "### Modelamiento del Lenguaje (De momento lo saltamos - Analizaremos en el dia 3-4)\n",
    "**Etapas 6 ~ 9: Relacionadas al modelamiento de lenguaje**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132a98c",
   "metadata": {},
   "source": [
    "## Reconocimiento del Habla (ASR) End-to-End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e30db8",
   "metadata": {},
   "source": [
    "### Etapa 10: Recoleccion de estadisticas ASR\n",
    "\n",
    "En esta etapa, estimamos la media y la varianza de nuestros datos para normalizarlos.  \n",
    "Tambien recogemos informacion acerca de las longitudes de entrada y salida para una\n",
    "generacion eficiente de los conjuntos de entrenamiento (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 10 --stop_stage 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b338af0",
   "metadata": {},
   "source": [
    "### Etapa 11: Entrenamiento del modelo\n",
    "Esta es la ejecucion principal del entrenamiento\n",
    "\n",
    "Monitorear los siguientes archivos:\n",
    "- Archivo de Registro: /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/train.log\n",
    "- Error: /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/images/loss.png\n",
    "- Exactitud: /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/images/acc.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma aproximadamente 20 a 30 minutos.\n",
    "!./run.sh --stage 11 --stop_stage 11 --ngpu 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac216d0",
   "metadata": {},
   "source": [
    "### Etapa 12: Decodificacion:\n",
    "En esta etapa evaluamos el modelo, generando texto de nuestras listas de evaluacion. \n",
    "Recuerda deshabilitar el uso del modelo de lenguaje, usando: `--use-lm false`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85168c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprox 10 mins.\n",
    "!./run.sh --inference_nj 4 --stage 12 --stop_stage 12 --use_lm false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63568c7f",
   "metadata": {},
   "source": [
    "### Etapa 13: Puntuacion\n",
    "Podemos encontrar el porcentaje de errores, por palabra (o WER), por caracter (CER), etc. para cada test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb676ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run.sh --stage 13 --stop_stage 13 --use_lm false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
